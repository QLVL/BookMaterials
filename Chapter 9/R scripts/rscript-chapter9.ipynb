{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load R-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(rjson)\n",
    "library(tidyverse)\n",
    "library(scales)\n",
    "library(emmeans)\n",
    "library(lmerTest)\n",
    "library(plotly)\n",
    "library(Rtsne)\n",
    "library(gridExtra)\n",
    "library(dbscan)\n",
    "\n",
    "library(reticulate)\n",
    "np <- import('numpy')\n",
    "\n",
    "options(repr.matrix.max.cols=70, repr.matrix.max.rows=100)\n",
    "options(repr.plot.width=20, repr.plot.height=20)\n",
    "\n",
    "`%ni%` = Negate(`%in%`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate and retrieve data\n",
    "\n",
    "The code in this notebook works if the underlying folder structure where data types are stored adheres to the following structure: \n",
    "\n",
    "* a parent folder (e.g.: '/project') \n",
    "* a first subfolder in which you only have subfolders per concept (e.g.: 'project/models/television'or 'project/models/shop')\n",
    "* in each concept folder you have three data types:\n",
    "    * the sampled tokens in a tsv file\n",
    "    * the cosine distance matrices created by means of the `nephosem` python module (where the matrices themselves are stored in npy-format and the metadata (i.e. the row- and column names) are stored separetely in meta-format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"YOUR/PATH/TO/DIRECTORY/project/models/\"\n",
    "\n",
    "ld <- list.dirs(path=path, full=F, recursive=F)\n",
    "concepts <- ld[! ld %in% c(\"tmp\", \".ipynb_checkpoints\",\"TEMPLATES\")]\n",
    "length(concepts) # how many concepts do I have? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Retrieve a dictionary (i.e. a Json object) with a concept and variants mapping\n",
    "\n",
    "Example\n",
    "\n",
    "`{\n",
    "'TELEVISION' : ['television', 'tv'],\n",
    " 'SHOP' : ['shop', 'store']\n",
    " }`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptsvariants = fromJSON(file = 'YOUR/PATH/TO/DIRECTORY/project/yourconceptsandvariantsdict.json')\n",
    "\n",
    "\n",
    "d.conceptsvariants = data.frame()\n",
    "for (n1 in names(conceptsvariants)) {\n",
    "    for (n2 in names(conceptsvariants[[n1]])) {\n",
    "        for (n3 in conceptsvariants[[n1]][[n2]]) {\n",
    "            d.conceptsvariants <- rbind(d.conceptsvariants, cbind(concept=n1,lemma=n2,wordform=n3))           \n",
    "        }\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieve a file with the token id's of the tokens that have been sampled to create the token-based models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startingTokens <- scan(file = \"YOUR/PATH/TO/DIRECTORY/project/sampled_tokens_chapter10.csv\", what = 'character', sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieve the files with the manual annotations\n",
    "\n",
    "In your project directory you should have a folder 'annotations' that contains the annotation files per concept. \n",
    "Such an annotation file is a text file (tsv or csv) with at least the following information:\n",
    "* a column with the token id's\n",
    "* a column with the wordform\n",
    "* a column with the concept annotation (i.e. 'yes' or 'no')\n",
    "\n",
    "Example\n",
    "\n",
    "| token-id | ... (extra columns with context)... | wordform | annotation |\n",
    "| --- | --- | --- | --- | \n",
    "| misstand;misstand/86-twitter-nl/39038 | middag ... # integriteit # defensiegate @ defensieonline - oorlogvoeren tegen eigen # personeel is verwerpelijk , een zeer ernstige;misstand;en # moreelondermijnend : meld # misstand bij # meldpunt sociaal veilige werkomgeving # defensie https://t.co/wrfsvsdihg https://t.co/lp07j2vqk7 @ nporadio2 er | misstand | yes |\n",
    "| misstand;misstand/74-twitter-nl/68806 | willen de wérkelijke problemen bij @ defensie kennen en helpen deze duurzaam op te lossen : meld daarom elke #;misstand;bij # meldpunt sociaal veilige werkomgeving # defensie https://t.co/wrfsvsdihg https://t.co/2ykwkclru9 @ axelosch @ vetleuk nee , deze gap is echt | misstand | no |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotationPath <- \"YOUR/PATH/TO/DIRECTORY/project/annotations/\"\n",
    "\n",
    "\n",
    "d.annotation <- data.frame()\n",
    "for (file in list.files(annotationPath)) {\n",
    "    annotationTemp <- read.csv2(paste0(annotationPath,file), header=T)\n",
    "    d.annotation <- rbind(d.annotation, annotationTemp)\n",
    "}\n",
    "\n",
    "rownames(d.annotation) <- d.annotation$Column2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of in-concept and out-of-concept tokens over the different concepts\n",
    "\n",
    "d.annotation2 <- merge(d.annotation, d.conceptsvariants, by.x=c(\"Column1\"), by.y=c(\"wordform\")) \n",
    "\n",
    "new.level.annotation2 <- d.annotation2 %>% \n",
    "group_by(concept, annotation) %>% \n",
    "summarize(n=n()) %>%\n",
    "mutate(freq = n / sum(n)) %>%\n",
    "filter(annotation == \"yes\") %>%\n",
    "arrange(desc(freq)) %>%\n",
    "pull(concept) %>%\n",
    "as.character()\n",
    "\n",
    "d.annotation2$concept <- factor(d.annotation2$concept, levels=new.level.annotation2)\n",
    "\n",
    "\n",
    "ggplot(d.annotation2, aes(x=annotation, y=concept)) + \n",
    "geom_bar(aes(x = (..count..)/tapply(..count.., ..y.. ,sum)[..y..], fill=annotation)) +\n",
    "scale_fill_manual(values=c(\"red\",\"green4\")) + \n",
    "theme(axis.text.x = element_text(size=20), \n",
    "      axis.text.y = element_text(size=20),\n",
    "      axis.title = element_text(size=20), \n",
    "      strip.text = element_text(size = 20),\n",
    "      legend.text = element_text(size=20),\n",
    "      legend.title = element_text(size= 20),\n",
    "      plot.title = element_text(size=20)) + \n",
    "scale_x_continuous(labels = scales::percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieve the file with the original size information of the concepts\n",
    "\n",
    "Example\n",
    "\n",
    "| concept | original_size | sample_size | proportion_sample |\n",
    "| --- | --- | --- | --- |\n",
    "| television | 73954 | 1400 | 0.02 |\n",
    "| shop | 8667 | 100 | 0.12 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd <- import(\"pandas\")\n",
    "concept_count_proportions <- pd$read_pickle(\"/home/stefano/CH10-diachronic-lectometry/concept_count_proportions\")\n",
    "# concept_count_proportions <- pd$read_pickle(\"YOUR/PATH/TO/DIRECTORY/project/concept_count_proportions\")\n",
    " \n",
    "colnames(concept_count_proportions) <- c(\"concept\", \"original_size\", \"sample_size\", \"proportion_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create plots for token clouds\n",
    "* static plots of the token clouds per model per concept\n",
    "* interactive plots of the token clouds per model per concept\n",
    "\n",
    "To avoid having tens or hundreds of plots, there are two options to limit this plotting procedure:\n",
    "* you only sample a random number of models for which you want to create plots of token clouds (e.g.: 9 or less)\n",
    "* you only sample 'good candidate' models to plot (which is based on the number of clusters [enough, but not too many], the presence of multiple annotated clusters, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsample = 9\n",
    "\n",
    "goodCandidates = TRUE\n",
    "\n",
    "plots_static1 <- list()\n",
    "plots_static2 <- list()\n",
    "plots_dynamic <- list()\n",
    "datasets <- list()\n",
    "\n",
    "for (con in concepts[1:2]) {\n",
    "  \n",
    "  all.models <- tools::file_path_sans_ext(list.files(path=paste0(path,con), pattern=\".*npy$\")) \n",
    "  all.models <- sample(all.models)\n",
    "  \n",
    "  if (goodCandidates == FALSE) {\n",
    "    \n",
    "    set.seed(1992)\n",
    "    selected.models <- sample(all.models,nsample)\n",
    "    \n",
    "    for (model in selected.models) { #concept_medoids[[css]][[con]]) {\n",
    "      \n",
    "      print(model)\n",
    "      flush.console()\n",
    "      \n",
    "      d <- np$load(paste0(path,con,\"/\",model,\".npy\"))\n",
    "      tokenids <- fromJSON(file = paste0(path,con,\"/\",model,\".meta\"))\n",
    "      \n",
    "      colnames(d) <- tokenids$row_items\n",
    "      rownames(d) <- tokenids$row_items\n",
    "      \n",
    "      tsvfile <- paste0(substr(model,1,nchar(model)-5),\".tsv\")\n",
    "      \n",
    "      # open and store dataframe\n",
    "      d.tsv <- read.csv(paste0(path,\"/\",con,\"/\",tsvfile), header=T, sep=\"\\t\", quote=\"\", check.names = F)\n",
    "      \n",
    "      # insert line breaks every 40 characters in the concordance of a token\n",
    "      # (in order to avoid long stretches of context in the visualization)\n",
    "      d.tsv$`_ctxt.model` <- gsub(\"(.{40,}?)\\\\s\", \"\\\\1<br>\", d.tsv$`_ctxt.model`)\n",
    "      # target token in bold face\n",
    "      d.tsv$`_ctxt.model` <- gsub(\"<span(<br>|\\\\s)class='target'>([^<]+)</span>\", \"<b>\\\\2</b>\", d.tsv$`_ctxt.model`)\n",
    "      # significant concontext words in italics\n",
    "      d.tsv$`_ctxt.model` <- gsub(\"<u>([^<]+)</u>\", \"<i>\\\\1</i>\", d.tsv$`_ctxt.model`)\n",
    "      # remove 'small font' tags for concontext words outside of concontext window\n",
    "      d.tsv$`_ctxt.model` <- gsub(\"</?small>\", \"\", d.tsv$`_ctxt.model`)\n",
    "      d.tsv$context <- d.tsv$`_ctxt.model`\n",
    "      \n",
    "      dsub <- d\n",
    "      \n",
    "      dmx1 <- as.matrix(dsub)   \n",
    "      ranks1 <- t(apply(dmx1, 1, rank))\n",
    "      ranks1 <- log(1 + log(ranks1))\n",
    "      dst <- as.dist(as.matrix(dist(ranks1, diag=T, upper=T)))\n",
    "      \n",
    "      set.seed(1992)\n",
    "      fitTSNE <- Rtsne(dst, dims=2, perplexity=30, theta=0.0, check.duplicates=FALSE, max_iter=1000, is_distance=TRUE)\n",
    "      tsne.x <- fitTSNE$Y[,1]\n",
    "      tsne.y <- fitTSNE$Y[,2]\n",
    "      \n",
    "      wordform <- gsub(\"^([^/]+)/.+\",\"\\\\1\",rownames(dsub),perl=T)\n",
    "      lemma <- wordform      \n",
    "      lemma <- as.factor(lemma)   \n",
    "      levels(lemma) <- as.character(d.conceptsvariants[which(d.conceptsvariants$wordform %in% levels(lemma)),'lemma'])\n",
    "      \n",
    "      lect <- gsub(\"^[^/]+/.*(nrc1999|nrc2000|stan2000|stan1999|twitter-nl|twitter-be|usenet-nl|usenet-be|nrc2017|standaard2017|standaard2018|nrc2018).+\",'\\\\1',rownames(dsub),perl=T)\n",
    "      country <- lect\n",
    "      formality <- lect\n",
    "      time <- lect\n",
    "      \n",
    "      country <- as.factor(country)\n",
    "      levels(country)[grepl(\"(nrc|-nl)\", levels(country))] <- \"NL-Du\"\n",
    "      levels(country)[grepl(\"(standaard|stan|-be)\", levels(country))] <- \"BE-Du\"\n",
    "      \n",
    "      formality <- as.factor(formality)\n",
    "      levels(formality)[grepl(\"(nrc|stan)\", levels(formality))] <- \"formal\"\n",
    "      levels(formality)[grepl(\"(twitter|usenet)\", levels(formality))] <- \"informal\"\n",
    "      \n",
    "      time <- as.factor(time)\n",
    "      levels(time)[grepl(\"(2000|1999|usenet)\", levels(time))] <- \"time1\"\n",
    "      levels(time)[grepl(\"(2017|2018|twitter)\", levels(time))] <- \"time2\"\n",
    "      \n",
    "      d.tsne <- data.frame(tsne.x, tsne.y, wordform=wordform, lemma=lemma, region=country, formality=formality, time=time)\n",
    "      rownames(d.tsne) <- rownames(dsub)\n",
    "      \n",
    "      d.tsne$annotation <- \"unk\"\n",
    "      sharedTokens <- intersect(rownames(d.tsne), rownames(d.annotation))\n",
    "      d.tsne[sharedTokens,\"annotation\"] <- as.character(d.annotation[sharedTokens,\"annotation\"])        \n",
    "      \n",
    "      d.tsne[as.character(d.tsv$`_id`),\"context\"] <- d.tsv$context\n",
    "      \n",
    "      hdbs.output <- hdbscan(dst, minPts = 8)\n",
    "      d.tsne$cluster.hdbs <- as.factor(as.character(hdbs.output$cluster))\n",
    "      d.tsne$membership <- hdbs.output$membership_prob\n",
    "      \n",
    "      d.tsne$index <- 1:nrow(d.tsne)\n",
    "      \n",
    "      datasets[[con]][[model]] <- d.tsne\n",
    "      \n",
    "      d.tsne$annotationNew <- as.factor(d.tsne$annotation)\n",
    "      d.tsne$annotationNew <- factor(d.tsne$annotationNew, levels = union(\"unk\", levels(as.factor(d.tsne$annotationNew)))) # c(\"unk\", \"no\",\"yes\"))\n",
    "      d.tsne$annotation <- factor(d.tsne$annotation, levels = union(\"unk\", levels(as.factor(d.tsne$annotation))))\n",
    "      levels(d.tsne$annotationNew) <- c(\"unk\", rep(\"ann\", nlevels(as.factor(d.tsne$annotationNew))-1))\n",
    "    \n",
    "      static.plot1 <- ggplot(d.tsne, aes(tsne.x,tsne.y)) + \n",
    "        geom_point(aes(color=cluster.hdbs, shape=annotation, text=paste('<b>Country</b>: ', region,\n",
    "                                                                        '</br><b>Word</b>: ', lemma,\n",
    "                                                                        '</br><b>Concontext</b>: ', context,\n",
    "                                                                        '</br><b>Token</b>: ', d.tsne$id)), \n",
    "                   stroke=1, size=(as.numeric(as.factor(d.tsne$annotationNew))**2)) + \n",
    "        scale_shape_manual(values=c(3,17,16)) +\n",
    "        scale_color_manual(values=c(\"grey\",hue_pal()(nlevels(d.tsne$cluster.hdbs)-1)))  + \n",
    "        theme_bw() + coord_fixed() + #ggtitle(model) + \n",
    "        theme(axis.title.x=element_blank(),\n",
    "              axis.title.y=element_blank(),\n",
    "              axis.text=element_text(size=12),\n",
    "              legend.title=element_text(size=12), \n",
    "              legend.text=element_text(size=12), \n",
    "              legend.position=\"right\", legend.box = \"vertical\", \n",
    "              plot.title=element_text(size=12)) + \n",
    "        guides(color = guide_legend(override.aes=list(size=10)),\n",
    "               shape = guide_legend(override.aes=list(size=10)))\n",
    "      \n",
    "      \n",
    "      plots_static1[[con]][[model]] <- static.plot1\n",
    "      \n",
    "      static.plot2 <- ggplot(d.tsne, aes(tsne.x,tsne.y)) + \n",
    "        geom_point(aes(color=lemma, shape=region, text=paste('<b>Country</b>: ', region,\n",
    "                                                             '</br><b>Word</b>: ', lemma,\n",
    "                                                             '</br><b>Concontext</b>: ', context,\n",
    "                                                             '</br><b>Token</b>: ', d.tsne$id)), size=2) + #, size=as.numeric(as.factor(d.tsne$annotationNew))**3) + \n",
    "        scale_shape_manual(values=c(4,16)) +\n",
    "        scale_color_manual(values=c(\"red\", \"black\"))  + \n",
    "        theme_bw() + coord_fixed() + #ggtitle(model) + \n",
    "        theme(axis.title.x=element_blank(),\n",
    "              axis.title.y=element_blank(),\n",
    "              axis.text=element_text(size=12),\n",
    "              legend.title=element_text(size=12), \n",
    "              legend.text=element_text(size=12), \n",
    "              legend.position=\"right\", legend.box = \"vertical\",\n",
    "              plot.title=element_text(size=12)) + \n",
    "        guides(color = guide_legend(override.aes=list(shape = \"—\", size=10)),\n",
    "               shape = guide_legend(override.aes=list(size=10)))\n",
    "      \n",
    "      \n",
    "      plots_static2[[con]][[model]] <- static.plot2\n",
    "      \n",
    "      plots_dynamic[[con]][[model]] <- ggplotly(static.plot2, tooltip = 'text', opacity=0.5) %>% layout(hoverlabel = list(bgcolor=c(\"rgb(255,255,204\")),\n",
    "                                                                                           xaxis = list(scaleanchor = \"y\", scaleratio = 1))  \n",
    "\n",
    "    }\n",
    "    \n",
    "  } else {\n",
    "    \n",
    "    ncandidates = 0  \n",
    "    \n",
    "    for (model in all.models) { \n",
    "      \n",
    "      print(model)\n",
    "      flush.console()\n",
    "      \n",
    "      d <- np$load(paste0(path,con,\"/\",model,\".npy\"))\n",
    "      tokenids <- fromJSON(file = paste0(path,con,\"/\",model,\".meta\"))\n",
    "      \n",
    "      colnames(d) <- tokenids$row_items\n",
    "      rownames(d) <- tokenids$row_items\n",
    "      \n",
    "      tsvfile <- paste0(substr(model,1,nchar(model)-5),\".tsv\")\n",
    "      \n",
    "      # open and store dataframe\n",
    "      d.tsv <- read.csv(paste0(path,\"/\",con,\"/\",tsvfile), header=T, sep=\"\\t\", quote=\"\", check.names = F)\n",
    "      \n",
    "      # insert line breaks every 40 characters in the concordance of a token\n",
    "      # (in order to avoid long stretches of context in the visualization)\n",
    "      d.tsv$`_ctxt.model` <- gsub(\"(.{40,}?)\\\\s\", \"\\\\1<br>\", d.tsv$`_ctxt.model`)\n",
    "      # target token in bold face\n",
    "      d.tsv$`_ctxt.model` <- gsub(\"<span(<br>|\\\\s)class='target'>([^<]+)</span>\", \"<b>\\\\2</b>\", d.tsv$`_ctxt.model`)\n",
    "      # significant concontext words in italics\n",
    "      d.tsv$`_ctxt.model` <- gsub(\"<u>([^<]+)</u>\", \"<i>\\\\1</i>\", d.tsv$`_ctxt.model`)\n",
    "      # remove 'small font' tags for concontext words outside of concontext window\n",
    "      d.tsv$`_ctxt.model` <- gsub(\"</?small>\", \"\", d.tsv$`_ctxt.model`)\n",
    "      d.tsv$context <- d.tsv$`_ctxt.model`\n",
    "      \n",
    "      dsub <- d\n",
    "      \n",
    "      dmx1 <- as.matrix(dsub)   \n",
    "      ranks1 <- t(apply(dmx1, 1, rank))\n",
    "      ranks1 <- log(1 + log(ranks1))\n",
    "      dst <- as.dist(as.matrix(dist(ranks1, diag=T, upper=T)))\n",
    "      \n",
    "      set.seed(1992)\n",
    "      fitTSNE <- Rtsne(dst, dims=2, perplexity=30, theta=0.0, check.duplicates=FALSE, max_iter=1000, is_distance=TRUE)\n",
    "      tsne.x <- fitTSNE$Y[,1]\n",
    "      tsne.y <- fitTSNE$Y[,2]\n",
    "      \n",
    "      wordform <- gsub(\"^([^/]+)/.+\",\"\\\\1\",rownames(dsub),perl=T)\n",
    "      lemma <- wordform      \n",
    "      lemma <- as.factor(lemma)   \n",
    "      levels(lemma) <- as.character(d.conceptsvariants[which(d.conceptsvariants$wordform %in% levels(lemma)),'lemma'])\n",
    "      \n",
    "      lect <- gsub(\"^[^/]+/.*(nrc1999|nrc2000|stan2000|stan1999|twitter-nl|twitter-be|usenet-nl|usenet-be|nrc2017|standaard2017|standaard2018|nrc2018).+\",'\\\\1',rownames(dsub),perl=T)\n",
    "      country <- lect\n",
    "      formality <- lect\n",
    "      time <- lect\n",
    "      \n",
    "      country <- as.factor(country)\n",
    "      levels(country)[grepl(\"(nrc|-nl)\", levels(country))] <- \"NL-Du\"\n",
    "      levels(country)[grepl(\"(standaard|stan|-be)\", levels(country))] <- \"BE-Du\"\n",
    "      \n",
    "      formality <- as.factor(formality)\n",
    "      levels(formality)[grepl(\"(nrc|stan)\", levels(formality))] <- \"formal\"\n",
    "      levels(formality)[grepl(\"(twitter|usenet)\", levels(formality))] <- \"informal\"\n",
    "      \n",
    "      time <- as.factor(time)\n",
    "      levels(time)[grepl(\"(2000|1999|usenet)\", levels(time))] <- \"time1\"\n",
    "      levels(time)[grepl(\"(2017|2018|twitter)\", levels(time))] <- \"time2\"\n",
    "      \n",
    "      d.tsne <- data.frame(tsne.x, tsne.y, wordform=wordform, lemma=lemma, region=country, formality=formality, time=time)\n",
    "      rownames(d.tsne) <- rownames(dsub)\n",
    "      \n",
    "      d.tsne$annotation <- \"unk\"\n",
    "      sharedTokens <- intersect(rownames(d.tsne), rownames(d.annotation))\n",
    "      d.tsne[sharedTokens,\"annotation\"] <- as.character(d.annotation[sharedTokens,\"annotation\"])        \n",
    "      \n",
    "      d.tsne[as.character(d.tsv$`_id`),\"context\"] <- d.tsv$context\n",
    "      \n",
    "      hdbs.output <- hdbscan(dst, minPts = 8)\n",
    "      d.tsne$cluster.hdbs <- as.factor(as.character(hdbs.output$cluster))\n",
    "      d.tsne$membership <- hdbs.output$membership_prob\n",
    "      \n",
    "      d.tsne$index <- 1:nrow(d.tsne)\n",
    "      \n",
    "      d.tsne$annotationNew <- as.factor(d.tsne$annotation)\n",
    "      d.tsne$annotationNew <- factor(d.tsne$annotationNew, levels = union(\"unk\", levels(as.factor(d.tsne$annotationNew)))) # c(\"unk\", \"no\",\"yes\"))\n",
    "      d.tsne$annotation <- factor(d.tsne$annotation, levels = union(\"unk\", levels(as.factor(d.tsne$annotation))))\n",
    "      levels(d.tsne$annotationNew) <- c(\"unk\", rep(\"ann\", nlevels(as.factor(d.tsne$annotationNew))-1))\n",
    "      \n",
    "      t <- table(d.tsne$cluster.hdbs, d.tsne$annotation)\n",
    "      t.ann <- t[,colnames(t) != \"unk\"]\n",
    "      \n",
    "      if (\n",
    "        # length(colnames(t)) == 3 & \n",
    "        nrow(t) <= 10 & # maximum 10 clusters\n",
    "        nrow(t) >= 4  # minimum 3 annotated clusters, not counting 'noise'\n",
    "        # prop.table(table(d.tsne$annotation)[c(\"yes\", \"no\")])[\"no\"] >= 0.2 # more than 19% out-of-concept tokens overall\n",
    "      ){\n",
    "        \n",
    "        if (!is.null(ncol(t.ann))) {\n",
    "          t.ann.sub <- t.ann[rowSums(t.ann) >= 5,]\n",
    "            if (!is.null(nrow(t.ann.sub))) {\n",
    "                propt.ann.sub <- prop.table(t.ann.sub, 1)\n",
    "            } else {\n",
    "                propt.ann.sub <- prop.table(t.ann.sub)\n",
    "            } \n",
    "        } else {\n",
    "          t.ann.sub <- t.ann[t.ann >= 5] \n",
    "          propt.ann.sub <- prop.table(t.ann.sub)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        datasets[[con]][[model]] <- d.tsne\n",
    "        \n",
    "        ncandidates <- ncandidates+ 1\n",
    "        if (ncandidates > 9) {\n",
    "          break\n",
    "        }\n",
    "        \n",
    "        static.plot1 <- ggplot(d.tsne, aes(tsne.x,tsne.y)) + \n",
    "          geom_point(aes(color=cluster.hdbs, shape=annotation, text=paste('<b>Country</b>: ', region,\n",
    "                                                                          '</br><b>Word</b>: ', lemma,\n",
    "                                                                          '</br><b>Concontext</b>: ', context,\n",
    "                                                                          '</br><b>Token</b>: ', d.tsne$id)), \n",
    "                     stroke=1, size=(as.numeric(as.factor(d.tsne$annotationNew))**2)) + \n",
    "          scale_shape_manual(values=c(3,17,16)) +\n",
    "          scale_color_manual(values=c(\"grey\",hue_pal()(nlevels(d.tsne$cluster.hdbs)-1)))  + \n",
    "          theme_bw() + coord_fixed() + #ggtitle(model) + \n",
    "          theme(axis.title.x=element_blank(),\n",
    "                axis.title.y=element_blank(),\n",
    "                axis.text=element_text(size=12),\n",
    "                legend.title=element_text(size=12), \n",
    "                legend.text=element_text(size=12),   \n",
    "                legend.position=\"right\", legend.box = \"vertical\", \n",
    "                plot.title=element_text(size=12)) + \n",
    "          guides(color = guide_legend(override.aes=list(size=10)),\n",
    "                 shape = guide_legend(override.aes=list(size=10)))\n",
    "        \n",
    "        \n",
    "        plots_static1[[con]][[model]] <- static.plot1\n",
    "        \n",
    "        static.plot2 <- ggplot(d.tsne, aes(tsne.x,tsne.y)) + \n",
    "          geom_point(aes(color=lemma, shape=region, text=paste('<b>Country</b>: ', region,\n",
    "                                                               '</br><b>Word</b>: ', lemma,\n",
    "                                                               '</br><b>Concontext</b>: ', context,\n",
    "                                                               '</br><b>Token</b>: ', d.tsne$id)), size=2) + #, size=as.numeric(as.factor(d.tsne$annotationNew))**3) + \n",
    "          scale_shape_manual(values=c(4,16)) +\n",
    "          scale_color_manual(values=c(\"red\", \"black\"))  + \n",
    "          theme_bw() + coord_fixed() + #ggtitle(model) + \n",
    "          theme(axis.title.x=element_blank(),\n",
    "                axis.title.y=element_blank(),\n",
    "                axis.text=element_text(size=12),\n",
    "                legend.title=element_text(size=12), \n",
    "                legend.text=element_text(size=12), \n",
    "                legend.position=\"right\", legend.box = \"vertical\",\n",
    "                plot.title=element_text(size=12)) + \n",
    "          guides(color = guide_legend(override.aes=list(shape = \"—\", size=10)),\n",
    "                 shape = guide_legend(override.aes=list(size=10)))\n",
    "        \n",
    "        \n",
    "        plots_static2[[con]][[model]] <- static.plot2\n",
    "          \n",
    "        plots_dynamic[[con]][[model]] <- ggplotly(static.plot2, tooltip = 'text', opacity=0.5) %>% layout(hoverlabel = list(bgcolor=c(\"rgb(255,255,204\")),\n",
    "                                                                                           xaxis = list(scaleanchor = \"y\", scaleratio = 1))  \n",
    "        \n",
    "      }    \n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do.call(\"grid.arrange\", c(plots_static2[['abuse']], ncol=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do.call(\"grid.arrange\", c(plots_static1[['abuse']], ncol=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the different steps for the creation of onomasiological profiles\n",
    "(this loop can take several hours, depending on the number of concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_datasets_list <- list()\n",
    "d.concepts <- data.frame()\n",
    "\n",
    "for (concept in concepts[1:5]) {\n",
    "  \n",
    "  conceptName <- concept\n",
    "  d.conceptsvariants.sub <- droplevels(subset(d.conceptsvariants, concept == conceptName))                                        \n",
    "  \n",
    "  lemmasSlash <- paste0(as.character(unlist(conceptsvariants[[concept]])),\"/\")\n",
    "  Pattern <- paste0(\"^\",lemmasSlash, collapse=\"|\")\n",
    "  allTokens <- startingTokens[grepl(Pattern, startingTokens)]\n",
    "  \n",
    "  d.ann.sub <- droplevels(subset(d.annotation2, concept == conceptName))\n",
    "  rownames(d.ann.sub) <- d.ann.sub$Column2\n",
    "\n",
    "  all.models <- tools::file_path_sans_ext(list.files(path=paste0(path,concept), pattern=\".*npy$\"))\n",
    "  \n",
    "  for (model in all.models) {\n",
    "    \n",
    "    # --------------------------------------------------- \n",
    "    # STEP 1: ALL DISAMBIGUATED TOKENS (1 MEASUREMENT PER CONCEPT)\n",
    "    print(\"step 1\")\n",
    "    # ---------------------------------------------------\n",
    "    \n",
    "    wordform <- gsub(\"^([^/]+)/.+\",\"\\\\1\",allTokens,perl=T)\n",
    "    lemma <- wordform      \n",
    "    lemma <- as.factor(lemma)   \n",
    "    levels(lemma) <- as.character(d.conceptsvariants.sub[which(d.conceptsvariants.sub$wordform %in% levels(lemma)),'lemma'])\n",
    "    \n",
    "    lect <- gsub(\"^[^/]+/.*(nrc1999|nrc2000|stan2000|stan1999|twitter-nl|twitter-be|usenet-nl|usenet-be|nrc2017|standaard2017|standaard2018|nrc2018).+\",'\\\\1',allTokens,perl=T)\n",
    "    country <- lect\n",
    "    formality <- lect\n",
    "    time <- lect\n",
    "    \n",
    "    country <- as.factor(country)\n",
    "    levels(country)[grepl(\"(nrc|-nl)\", levels(country))] <- \"Netherlands\"\n",
    "    levels(country)[grepl(\"(standaard|stan|-be)\", levels(country))] <- \"Belgium\"\n",
    "    country <- factor(country, levels=c(\"Belgium\",\"Netherlands\"))\n",
    "    \n",
    "    formality <- as.factor(formality)\n",
    "    levels(formality)[grepl(\"(nrc|stan)\", levels(formality))] <- \"formal\"\n",
    "    levels(formality)[grepl(\"(twitter|usenet)\", levels(formality))] <- \"informal\"\n",
    "    \n",
    "    time <- as.factor(time)\n",
    "    levels(time)[grepl(\"(2000|1999|usenet)\", levels(time))] <- \"time1\"\n",
    "    levels(time)[grepl(\"(2017|2018|twitter)\", levels(time))] <- \"time2\"\n",
    "    \n",
    "    d.alltokens <- data.frame(allTokens, wordform, lemma, lect, country, formality, time)\n",
    "    \n",
    "    # ALL\n",
    "    t.final <- table(lemma, country) \n",
    "    prop.proft <- round(prop.table(t.final, 2),10)\n",
    "    \n",
    "    u <- round(sum(apply(prop.proft, 1, min), na.rm=T), 10)\n",
    "    iu <- round(sum(prop.table(rowSums(t.final))^2), 10)\n",
    "    lect1.iu <- round(sum(prop.table(t.final[,1])^2), 10)\n",
    "    lect2.iu <- round(sum(prop.table(t.final[,2])^2), 10)\n",
    "    tot.tokens <- sum(t.final)\n",
    "    \n",
    "    step <- \"all.disambiguated.tokens\" \n",
    "    \n",
    "    d.concepts <- bind_rows(d.concepts, cbind.data.frame(model, concept, step, u, iu, lect1.iu, lect2.iu, tot.tokens))\n",
    "    \n",
    "    # ----------------------------\n",
    "    \n",
    "    cw = strsplit(model,\"\\\\.\")[[1]][2]\n",
    "    foc_contribution = strsplit(model,\"\\\\.\")[[1]][3]\n",
    "    ass_foc = strsplit(model,\"\\\\.\")[[1]][4]\n",
    "    soc_selection = strsplit(model,\"\\\\.\")[[1]][5]\n",
    "    svd = strsplit(model,\"\\\\.\")[[1]][6]\n",
    "    \n",
    "    print(model)\n",
    "    flush.console()\n",
    "    \n",
    "    d <- np$load(paste0(path,concept,\"/\",model,\".npy\"))\n",
    "    tokenids <- fromJSON(file = paste0(path,concept,\"/\",model,\".meta\"))\n",
    "    \n",
    "    colnames(d) <- tokenids$row_items\n",
    "    rownames(d) <- tokenids$row_items\n",
    "    \n",
    "    tsvfile <- paste0(substr(model,1,nchar(model)-5),\".tsv\")\n",
    "    \n",
    "    # open and store dataframe\n",
    "    d.context <- read.csv(paste0(path,\"/\",concept,\"/\",tsvfile), header=T, sep=\"\\t\", quote=\"\", check.names = F)\n",
    "    \n",
    "    rownames(d.context) <- d.context$`_id`\n",
    "    d.context$lemma <- d.context$word\n",
    "    levels(d.context$lemma) <- as.character(d.conceptsvariants.sub[which(d.conceptsvariants.sub$wordform %in% levels(d.context$word)),'lemma'])\n",
    "    \n",
    "    subTokens <- intersect(rownames(d.context), rownames(d))\n",
    "    \n",
    "    dsub <- d[subTokens, subTokens] \n",
    "      d.context$in.out <- d.ann.sub[match(subTokens, as.character(d.context$`_id`)),\"annotation\"] \n",
    "      d.context$in.out <- as.character(d.context$in.out)\n",
    "      d.context$in.out = d.context$in.out %>% replace_na('unk')\n",
    "      d.context$in.out <- as.factor(d.context$in.out)\n",
    "    \n",
    "    dmx1 <- as.matrix(dsub) \n",
    "    ranks1 <- t(apply(dmx1, 1, rank))\n",
    "    ranks1 <- log(1 + log(ranks1))\n",
    "    dst <- as.dist(as.matrix(dist(ranks1, diag=T, upper=T)))\n",
    "    \n",
    "    # density-based cluster analysis \n",
    "    hdbs.output <- hdbscan(dst, minPts = 8)\n",
    "    d.context$cluster.hdbs <- as.factor(as.character(hdbs.output$cluster))\n",
    "    \n",
    "    d.context$membership <- hdbs.output$membership_prob\n",
    "    \n",
    "    # --------------------------------------------------- \n",
    "    # STEP 2: MODELLED DISAMBIGUATED TOKENS (1 MEASURMENT PER MODEL)\n",
    "    print(\"step 2\")\n",
    "    # ---------------------------------------------------\n",
    "    \n",
    "    # ALL\n",
    "    t.final <- table(d.context$lemma, d.context$country) \n",
    "    prop.proft <- round(prop.table(t.final, 2),10)\n",
    "    \n",
    "    u <- round(sum(apply(prop.proft, 1, min), na.rm=T), 10)\n",
    "    iu <- round(sum(prop.table(rowSums(t.final))^2), 10)\n",
    "    lect1.iu <- round(sum(prop.table(t.final[,1])^2), 10)\n",
    "    lect2.iu <- round(sum(prop.table(t.final[,2])^2), 10)\n",
    "    tot.tokens <- sum(t.final)\n",
    "    \n",
    "    step <- \"modelled.disambiguated.tokens\" \n",
    "    d.concepts <- bind_rows(d.concepts, cbind.data.frame(model, concept, step, u, iu, lect1.iu, lect2.iu, tot.tokens))\n",
    "    \n",
    "    # --------------------------------------------------- \n",
    "    # STEP 3: ALL MODELLED TOKENS EXCEPT FOR NOISE TOKENS\n",
    "    print(\"step 3\")\n",
    "    # ---------------------------------------------------\n",
    "    \n",
    "    # ALL \n",
    "    d.context.nonoise.all <- droplevels(subset(d.context, cluster.hdbs != 0))\n",
    "    \n",
    "    if (nrow(d.context.nonoise.all) == 0){\n",
    "        \n",
    "        u <- NA\n",
    "        iu <- NA\n",
    "        lect1.iu <- NA\n",
    "        lect2.iu <- NA\n",
    "        tot.tokens <- 0\n",
    "        \n",
    "        step <- \"nonoise\" \n",
    "        d.concepts <- bind_rows(d.concepts, cbind.data.frame(model, concept, step, u, iu, lect1.iu, lect2.iu, tot.tokens))\n",
    "        step <- \"nonoise.nomonolect\" \n",
    "        d.concepts <- bind_rows(d.concepts, cbind.data.frame(model, concept, step, u, iu, lect1.iu, lect2.iu, tot.tokens))\n",
    "        step <- \"nonoise.nomonolectal.noout\" \n",
    "        d.concepts <- bind_rows(d.concepts, cbind.data.frame(model, concept, step, u, iu, lect1.iu, lect2.iu, tot.tokens))\n",
    "\n",
    "    } else {\n",
    "      t.final <- table(d.context.nonoise.all$lemma, d.context.nonoise.all$country) \n",
    "      prop.proft <- round(prop.table(t.final, 2),10)\n",
    "      \n",
    "      u <- round(sum(apply(prop.proft, 1, min), na.rm=T), 10)\n",
    "      iu <- round(sum(prop.table(rowSums(t.final))^2), 10)\n",
    "      lect1.iu <- round(sum(prop.table(t.final[,1])^2), 10)\n",
    "      lect2.iu <- round(sum(prop.table(t.final[,2])^2), 10)\n",
    "      tot.tokens <- sum(t.final)        \n",
    "      \n",
    "      step <- \"nonoise\" \n",
    "      d.concepts <- bind_rows(d.concepts, cbind.data.frame(model, concept, step, u, iu, lect1.iu, lect2.iu, tot.tokens))\n",
    "      \n",
    "      # --------------------------------------------------- \n",
    "      # STEP 4: ALL MODELLED TOKENS EXCEPT FOR NOISE TOKENS AND EXCEPT FOR MONOLECTAL CLUSTERS\n",
    "      print(\"step 4\")\n",
    "      # ---------------------------------------------------\n",
    "      \n",
    "      t.monolectal <- table(d.context.nonoise.all$cluster.hdbs, d.context.nonoise.all$country) \n",
    "      monolectal.clusters <- rownames(t.monolectal)[rowSums(t.monolectal == 0) == nlevels(d.context.nonoise.all$country) - 1] \n",
    "      \n",
    "      # ALL \n",
    "      d.context.nonoise.nomonolectal.all <- droplevels(subset(d.context.nonoise.all, cluster.hdbs %ni% monolectal.clusters))\n",
    "      # write.table(d.context.nonoise.nomonolectal.all, paste0(\"/home/projects/semmetrix/NephoSem/output-data/new-chapter10/models/\",concept,\"/\",model,\".nonoise.nomonolectal.tsv\"), row.names=F, quote=F, sep=\"\\t\")\n",
    "      \n",
    "      t.final <- table(d.context.nonoise.nomonolectal.all$lemma, d.context.nonoise.nomonolectal.all$country) \n",
    "      prop.proft <- round(prop.table(t.final, 2),10)\n",
    "      \n",
    "      u <- round(sum(apply(prop.proft, 1, min), na.rm=T), 10)\n",
    "      iu <- round(sum(prop.table(rowSums(t.final))^2), 10)\n",
    "      lect1.iu <- round(sum(prop.table(t.final[,1])^2), 10)\n",
    "      lect2.iu <- round(sum(prop.table(t.final[,2])^2), 10)\n",
    "      tot.tokens <- sum(t.final)\n",
    "      \n",
    "      step <- \"nonoise.nomonolect\" \n",
    "      d.concepts <- bind_rows(d.concepts, cbind.data.frame(model, concept, step, u, iu, lect1.iu, lect2.iu, tot.tokens))\n",
    "      \n",
    "      \n",
    "      \n",
    "      # --------------------------------------------------- \n",
    "      # STEP 5: ALL MODELLED TOKENS EXCEPT FOR NOISE TOKENS, EXCEPT FOR MONOLECTAL CLUSTERS AND\n",
    "      #         EXCEPT FOR OUT-OF-CONCEPT CLUSTERS\n",
    "      print(\"step 5\")\n",
    "      # ---------------------------------------------------\n",
    "      \n",
    "      tab <- table(d.context.nonoise.nomonolectal.all$cluster.hdbs, d.context.nonoise.nomonolectal.all$in.out)\n",
    "      disambiguatedColumns <- colnames(tab)[!(colnames(tab) %in% \"unk\")]\n",
    "      clustersWith10pDisambiguated <- rownames(tab[rowSums(tab[,disambiguatedColumns, drop=FALSE])/rowSums(tab) >= 0.1,])\n",
    "      clustersWith5tDisambiguated <- rownames(tab[rowSums(tab[,disambiguatedColumns, drop=FALSE]) >= 5,])\n",
    "      \n",
    "      clustersRetained <- unique(c(clustersWith10pDisambiguated, clustersWith5tDisambiguated))\n",
    "      \n",
    "      tabFinalClusters <- tab[clustersRetained,]\n",
    "      \n",
    "      if (nrow(tabFinalClusters) == 0 | \"yes\" %ni% colnames(tabFinalClusters)) {\n",
    "        in.clusters <- as.character()\n",
    "      } else {\n",
    "        pt <- prop.table(tabFinalClusters[,disambiguatedColumns, drop=FALSE],1)\n",
    "        in.clusters <- rownames(pt)[pt[,\"yes\"] >= 0.8]            \n",
    "      }\n",
    "     \n",
    "      if (length(in.clusters) != 0)  {\n",
    "        \n",
    "        # ALL \n",
    "        d.context.nonoise.noout.nomonolectal.all <- droplevels(subset(d.context.nonoise.nomonolectal.all, cluster.hdbs %in% in.clusters))\n",
    "        d.context.nonoise.noout.nomonolectal.all$model <- model\n",
    "        final_datasets_list[[concept]][[model]] <- d.context.nonoise.noout.nomonolectal.all\n",
    "        \n",
    "        \n",
    "        t.final <- table(d.context.nonoise.noout.nomonolectal.all$lemma, d.context.nonoise.noout.nomonolectal.all$country) \n",
    "        prop.proft <- round(prop.table(t.final, 2),10)\n",
    "        \n",
    "        u <- round(sum(apply(prop.proft, 1, min), na.rm=T), 10)\n",
    "        iu <- round(sum(prop.table(rowSums(t.final))^2), 10)\n",
    "        lect1.iu <- round(sum(prop.table(t.final[,1])^2), 10)\n",
    "        lect2.iu <- round(sum(prop.table(t.final[,2])^2), 10)\n",
    "        tot.tokens <- sum(t.final)\n",
    "        \n",
    "        step <- \"nonoise.nomonolectal.noout\" \n",
    "        d.concepts <- bind_rows(d.concepts, cbind.data.frame(model, concept, step, u, iu, lect1.iu, lect2.iu, tot.tokens))\n",
    "        \n",
    "      } else {\n",
    "        \n",
    "        u <- NA\n",
    "        iu <- NA\n",
    "        lect1.iu <- NA\n",
    "        lect2.iu <- NA\n",
    "        tot.tokens <- 0\n",
    "        \n",
    "        step <- \"nonoise.nomonolectal.noout\" \n",
    "        d.concepts <- bind_rows(d.concepts, cbind.data.frame(model, concept, step, u, iu, lect1.iu, lect2.iu, tot.tokens))\n",
    "        \n",
    "      }                     \n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many models are removed after the stepwise profile-forming procedure\n",
    "\n",
    "paste0(round(nrow(d.concepts[which(d.concepts$tot.tokens == 0 & d.concepts$step == \"nonoise.nomonolectal.noout\"),])/\n",
    "nrow(d.concepts[which(d.concepts$step == \"nonoise.nomonolectal.noout\"),])*100), \"%\")\n",
    "\n",
    "dKeptModels <- data.frame()\n",
    "for (con in concepts) {\n",
    "    \n",
    "    kept.models <- nrow(d.concepts[which(d.concepts$tot.tokens == 0 & d.concepts$step == \"nonoise.nomonolectal.noout\" & d.concepts$concept == con),])/\n",
    "    nrow(d.concepts[which(d.concepts$step == \"nonoise.nomonolectal.noout\" & d.concepts$concept == con),])\n",
    "    \n",
    "    cat(con,  paste0(round(kept.models*100),\"%\"),\"\\n\")\n",
    "    dKeptModels <- rbind(dKeptModels, cbind.data.frame(con, averageKeptModels = round(kept.models*100)))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many tokens on average are kept in the remaning models\n",
    "\n",
    "d.concepts[is.na(d.concepts$tot.tokens),\"tot.tokens\"] <- 0\n",
    "\n",
    "ggplot(d.concepts, aes(x=step, y=tot.tokens)) + \n",
    "geom_bar(stat=\"summary\", fun=\"mean\") + \n",
    "geom_errorbar(stat=\"summary\") + \n",
    "facet_wrap(~concept) + \n",
    "theme(axis.text.x = element_text(size=20, angle=45, vjust=0.5), \n",
    "      axis.text.y = element_text(size=20),\n",
    "      axis.title = element_text(size=20), \n",
    "      strip.text = element_text(size = 20),\n",
    "      legend.text = element_text(size=20),\n",
    "      legend.title = element_text(size=20),\n",
    "      plot.title = element_text(size=20))\n",
    "\n",
    "d.concepts %>%\n",
    "  group_by(concept) %>%\n",
    "  summarize(DiffPerc = mean(tot.tokens[step == \"nonoise.nomonolectal.noout\"]/tot.tokens[step == \"all.disambiguated.tokens\"])) %>%\n",
    "  arrange(desc(DiffPerc)) %>%\n",
    "  mutate(DiffPerc = scales::percent(DiffPerc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations of 'destandardization', 'informalization', 'dehomogeneization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.conceptsvariants$partofspeech <- \"noun\"\n",
    "d.conceptsvariants$semfield <- NA\n",
    "\n",
    "d.conceptsvariants[which(d.conceptsvariants$concept %in% levels(d.conceptsvariants$concept)[1:19]),]$partofspeech <- \"adjective\"\n",
    "d.conceptsvariants[which(d.conceptsvariants$concept %in% levels(d.conceptsvariants$concept)[20:25]),]$partofspeech <- \"adverb\"\n",
    "d.conceptsvariants[which(d.conceptsvariants$concept %in% levels(d.conceptsvariants$concept)[26:35]),]$partofspeech <- \"verb\"\n",
    "\n",
    "d.conceptsvariants[which(d.conceptsvariants$concept %in% levels(d.conceptsvariants$concept)[36:39]),]$semfield <- \"science\"\n",
    "d.conceptsvariants[which(d.conceptsvariants$concept %in% levels(d.conceptsvariants$concept)[40:44]),]$semfield <- \"sports\"\n",
    "d.conceptsvariants[which(d.conceptsvariants$concept %in% levels(d.conceptsvariants$concept)[45:53]),]$semfield <- \"economy\"\n",
    "d.conceptsvariants[which(d.conceptsvariants$concept %in% levels(d.conceptsvariants$concept)[54:64]),]$semfield <- \"politics\"\n",
    "d.conceptsvariants[which(d.conceptsvariants$concept %in% levels(d.conceptsvariants$concept)[65:76]),]$semfield <- \"abstract\"\n",
    "d.conceptsvariants[which(d.conceptsvariants$concept %in% levels(d.conceptsvariants$concept)[77:85]),]$semfield <- \"rest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.concepts.clusters.demo <- data.frame()\n",
    "\n",
    "for (concept in concepts) {\n",
    "    \n",
    "    print(concept)\n",
    "    flush.console()\n",
    "        \n",
    "     for (model in names(final_datasets_list[[concept]])) { # ) {   \n",
    "        d <- final_datasets_list[[concept]][[model]] \n",
    "    \n",
    "        d.cluster <- d\n",
    "          tot.tokens.cluster <- nrow(d.cluster)\n",
    "        \n",
    "          # destandardization Belgian-Dutch\n",
    "            d.bel.per1 <- droplevels(subset(d.cluster, country == \"Belgium\" & timeperiod == \"period1\"))\n",
    "            t.bel.per1 <- table(d.bel.per1$lemma, d.bel.per1$formality)\n",
    "            sum.t.bel.per1 <- sum(t.bel.per1)\n",
    "\n",
    "            \n",
    "            if (ncol(t.bel.per1) == 2) {\n",
    "              if (nrow(t.bel.per1) > 1) {\n",
    "                if (fisher.test(t.bel.per1)$p.value < 0.05) {\n",
    "                  prop.t.bel.per1 <- round(prop.table(t.bel.per1, 2),10)\n",
    "                  u.bel.per1 <- round(sum(apply(prop.t.bel.per1, 1, min), na.rm=T), 10)\n",
    "                } else {\n",
    "                  u.bel.per1 <- 1\n",
    "                }\n",
    "              } else {\n",
    "                u.bel.per1 <- 1\n",
    "              }\n",
    "            } else {\n",
    "              u.bel.per1 <- NA\n",
    "            }\n",
    "\n",
    "            d.bel.per2 <- droplevels(subset(d.cluster, country == \"Belgium\" & timeperiod == \"period2\"))\n",
    "            t.bel.per2 <- table(d.bel.per2$lemma, d.bel.per2$formality)\n",
    "            sum.t.bel.per2 <- sum(t.bel.per2)\n",
    "\n",
    "            \n",
    "            if (ncol(t.bel.per2) == 2) {\n",
    "              if (nrow(t.bel.per2) > 1) {\n",
    "                if (fisher.test(t.bel.per2)$p.value < 0.05) {\n",
    "                  prop.t.bel.per2 <- round(prop.table(t.bel.per2, 2),10)\n",
    "                  u.bel.per2 <- round(sum(apply(prop.t.bel.per2, 1, min), na.rm=T), 10)\n",
    "                } else {\n",
    "                  u.bel.per2 <- 1\n",
    "                }\n",
    "              } else {\n",
    "                u.bel.per2 <- 1\n",
    "              }\n",
    "            } else {\n",
    "              u.bel.per2 <- NA\n",
    "            }\n",
    "  \n",
    "\n",
    "         \n",
    "          sum.t.bel <- sum.t.bel.per1 + sum.t.bel.per2\n",
    "        \n",
    "          # destandardization Netherlandic-Dutch\n",
    "            d.ned.per1 <- droplevels(subset(d.cluster, country == \"Netherlands\" & timeperiod == \"period1\"))\n",
    "            t.ned.per1 <- table(d.ned.per1$lemma, d.ned.per1$formality)\n",
    "            sum.t.ned.per1 <- sum(t.ned.per1)\n",
    "\n",
    "            if (ncol(t.ned.per1) == 2) {\n",
    "              if (nrow(t.ned.per1) > 1) {\n",
    "                if (fisher.test(t.ned.per1)$p.value < 0.05) {\n",
    "                  prop.t.ned.per1 <- round(prop.table(t.ned.per1, 2),10)\n",
    "                  u.ned.per1 <- round(sum(apply(prop.t.ned.per1, 1, min), na.rm=T), 10)\n",
    "                } else {\n",
    "                  u.ned.per1 <- 1\n",
    "                }\n",
    "              } else {\n",
    "                u.ned.per1 <- 1\n",
    "              }\n",
    "            } else {\n",
    "              u.ned.per1 <- NA\n",
    "            }\n",
    "\n",
    "            d.ned.per2 <- droplevels(subset(d.cluster, country == \"Netherlands\" & timeperiod == \"period2\"))\n",
    "            t.ned.per2 <- table(d.ned.per2$lemma, d.ned.per2$formality)\n",
    "            sum.t.ned.per2 <- sum(t.ned.per2)\n",
    "\n",
    "            if (ncol(t.ned.per2) == 2) {\n",
    "              if (nrow(t.ned.per2) > 1) {\n",
    "                if (fisher.test(t.ned.per2)$p.value < 0.05) {\n",
    "                  prop.t.ned.per2 <- round(prop.table(t.ned.per2, 2),10)\n",
    "                  u.ned.per2 <- round(sum(apply(prop.t.ned.per2, 1, min), na.rm=T), 10)\n",
    "                } else {\n",
    "                  u.ned.per2 <- 1\n",
    "                }\n",
    "              } else {\n",
    "                u.ned.per2 <- 1\n",
    "              }\n",
    "            } else {\n",
    "              u.ned.per2 <- NA\n",
    "            }\n",
    "         \n",
    "          sum.t.ned <- sum.t.ned.per1 + sum.t.ned.per2\n",
    " \n",
    "          # informalization Belgian-Dutch\n",
    "            d.bel <- subset(d.cluster, country == \"Belgium\")\n",
    "\n",
    "            if (nlevels(droplevels(d.bel$formality)) == 2 & nlevels(droplevels(d.bel$timeperiod)) == 2) {\n",
    "\n",
    "              beltab11 <- table(d.bel$lemma, d.bel$timeperiod, d.bel$formality)[,\"period1\",\"informal\"]\n",
    "              beltab12 <- table(d.bel$lemma, d.bel$timeperiod, d.bel$formality)[,\"period2\",\"formal\"]\n",
    "              belfintab1 <- matrix(c(beltab11, beltab12), nrow=length(beltab12))\n",
    "\n",
    "            if (sum(belfintab1) != 0) {\n",
    "              if (nrow(belfintab1) > 1) {\n",
    "                if (fisher.test(belfintab1)$p.value < 0.05) {\n",
    "\n",
    "                  bel.inf.per1 <- prop.table(table(d.bel$lemma, d.bel$timeperiod, d.bel$formality),c(2,3))[,\"period1\",\"informal\"]\n",
    "                  bel.for.per2 <- prop.table(table(d.bel$lemma, d.bel$timeperiod, d.bel$formality),c(2,3))[,\"period2\",\"formal\"]\n",
    "                  prop.t.bel.1 <- matrix(c(bel.inf.per1, bel.for.per2), nrow=length(bel.for.per2))\n",
    "                  u.bel.inf.for.1 <- round(sum(apply(prop.t.bel.1, 1, min)), 10) \n",
    "\n",
    "                } else {\n",
    "                  u.bel.inf.for.1 <- 1\n",
    "                }\n",
    "              } else {\n",
    "                u.bel.inf.for.1 <- 1\n",
    "              }\n",
    "            } else {\n",
    "              u.bel.inf.for.1 <- NA\n",
    "            }\n",
    "\n",
    "              beltab21 <- table(d.bel$lemma, d.bel$timeperiod, d.bel$formality)[,\"period1\",\"formal\"]\n",
    "              beltab22 <- table(d.bel$lemma, d.bel$timeperiod, d.bel$formality)[,\"period2\",\"informal\"]\n",
    "              belfintab2 <- matrix(c(beltab21, beltab22), nrow=length(beltab21))\n",
    "\n",
    "            if (sum(belfintab2) != 0) {\n",
    "              if (nrow(belfintab2) > 1) {\n",
    "                if (fisher.test(belfintab2)$p.value < 0.05) {\n",
    "\n",
    "                  bel.for.per1 <- prop.table(table(d.bel$lemma, d.bel$timeperiod, d.bel$formality),c(2,3))[,\"period1\",\"formal\"]\n",
    "                  bel.inf.per2 <- prop.table(table(d.bel$lemma, d.bel$timeperiod, d.bel$formality),c(2,3))[,\"period2\",\"informal\"]\n",
    "                  prop.t.bel.2 <- matrix(c(bel.for.per1, bel.inf.per2), nrow=length(bel.inf.per2))\n",
    "                  u.bel.inf.for.2 <- round(sum(apply(prop.t.bel.2, 1, min)), 10)\n",
    "\n",
    "                } else {\n",
    "                  u.bel.inf.for.2 <- 1\n",
    "                }\n",
    "              } else {\n",
    "                u.bel.inf.for.2 <- 1\n",
    "              }\n",
    "            } else {\n",
    "              u.bel.inf.for.2 <- NA\n",
    "            }\n",
    "\n",
    "            } else {\n",
    "              u.bel.inf.for.1 <- NA\n",
    "              u.bel.inf.for.2 <- NA\n",
    "            }         \n",
    "        \n",
    "          # informalization Netherlandic-Dutch\n",
    "            d.ned <- subset(d.cluster, country == \"Netherlands\")\n",
    "\n",
    "            if (nlevels(droplevels(d.ned$formality)) == 2 & nlevels(droplevels(d.ned$timeperiod)) == 2) {\n",
    "\n",
    "              nedtab11 <- table(d.ned$lemma, d.ned$timeperiod, d.ned$formality)[,\"period1\",\"informal\"]\n",
    "              nedtab12 <- table(d.ned$lemma, d.ned$timeperiod, d.ned$formality)[,\"period2\",\"formal\"]\n",
    "              nedfintab1 <- matrix(c(nedtab11, nedtab12), nrow=length(nedtab12))\n",
    "        \n",
    "              if (sum(nedfintab1) != 0) {\n",
    "                if (nrow(nedfintab1) > 1) {\n",
    "                  if (fisher.test(nedfintab1)$p.value < 0.05) {\n",
    "\n",
    "                    ned.inf.per1 <- prop.table(table(d.ned$lemma, d.ned$timeperiod, d.ned$formality),c(2,3))[,\"period1\",\"informal\"]\n",
    "                    ned.for.per2 <- prop.table(table(d.ned$lemma, d.ned$timeperiod, d.ned$formality),c(2,3))[,\"period2\",\"formal\"]\n",
    "                    prop.t.ned.1 <- matrix(c(ned.inf.per1, ned.for.per2), nrow=length(ned.for.per2))\n",
    "                    u.ned.inf.for.1 <- round(sum(apply(prop.t.ned.1, 1, min)), 10) \n",
    "\n",
    "                  } else {\n",
    "                    u.ned.inf.for.1 <- 1\n",
    "                  }\n",
    "                } else {\n",
    "                  u.ned.inf.for.1 <- 1\n",
    "                }\n",
    "              } else {\n",
    "                u.ned.inf.for.1 <- NA\n",
    "              }\n",
    "\n",
    "              nedtab21 <- table(d.ned$lemma, d.ned$timeperiod, d.ned$formality)[,\"period1\",\"formal\"]\n",
    "              nedtab22 <- table(d.ned$lemma, d.ned$timeperiod, d.ned$formality)[,\"period2\",\"informal\"]\n",
    "              nedfintab2 <- matrix(c(nedtab21, nedtab22), nrow=length(nedtab21))\n",
    "\n",
    "              if (sum(nedfintab2) != 0) {\n",
    "                if (nrow(nedfintab2) > 1) {\n",
    "                  if (fisher.test(nedfintab2)$p.value < 0.05) {\n",
    "\n",
    "                    ned.for.per1 <- prop.table(table(d.ned$lemma, d.ned$timeperiod, d.ned$formality),c(2,3))[,\"period1\",\"formal\"]\n",
    "                    ned.inf.per2 <- prop.table(table(d.ned$lemma, d.ned$timeperiod, d.ned$formality),c(2,3))[,\"period2\",\"informal\"]\n",
    "                    prop.t.ned.2 <- matrix(c(ned.for.per1, ned.inf.per2), nrow=length(ned.inf.per2))\n",
    "                    u.ned.inf.for.2 <- round(sum(apply(prop.t.ned.2, 1, min)), 10)\n",
    "\n",
    "                  } else {\n",
    "                    u.ned.inf.for.2 <- 1\n",
    "                  }\n",
    "                } else {\n",
    "                  u.ned.inf.for.2 <- 1\n",
    "                }\n",
    "              } else {\n",
    "                u.ned.inf.for.2 <- NA\n",
    "              }\n",
    "\n",
    "            } else {\n",
    "              u.ned.inf.for.1 <- NA\n",
    "              u.ned.inf.for.2 <- NA\n",
    "            } \n",
    "         \n",
    "          # dehomogeneization Belgian-Dutch\n",
    "          iu.bel.formal.per1 <-  tryCatch(round(sum(prop.table(t.bel.per1[,\"formal\"])^2), 10),  error=function(err) NA)\n",
    "          iu.bel.formal.per2 <- tryCatch(round(sum(prop.table(t.bel.per2[,\"formal\"])^2), 10),  error=function(err) NA)\n",
    "\n",
    "          # dehomogeneization Netherlandic-Dutch\n",
    "          iu.ned.formal.per1 <- tryCatch(round(sum(prop.table(t.ned.per1[,\"formal\"])^2), 10),  error=function(err) NA)\n",
    "          iu.ned.formal.per2 <- tryCatch(round(sum(prop.table(t.ned.per2[,\"formal\"])^2), 10),  error=function(err) NA)\n",
    "        \n",
    "         \n",
    "        cw = strsplit(model,\"\\\\.\")[[1]][2]\n",
    "        foc_contribution = strsplit(model,\"\\\\.\")[[1]][3]\n",
    "        ass_foc = strsplit(model,\"\\\\.\")[[1]][4]\n",
    "        soc_selection = strsplit(model,\"\\\\.\")[[1]][5]\n",
    "        svd = strsplit(model,\"\\\\.\")[[1]][6]\n",
    "        foc_selection = strsplit(model,\"\\\\.\")[[1]][7]\n",
    "\n",
    "          d.concepts.clusters.demo <-   bind_rows(d.concepts.clusters.demo, cbind.data.frame(model.name=model, cw, foc_contribution, ass_foc, soc_selection, svd, foc_selection, concept, \n",
    "                                                                                             tot.tokens.cluster, freq.bel.per1 = sum.t.bel.per1, freq.bel.per2 = sum.t.bel.per2,\n",
    "                                                                                             freq.ned.per1 = sum.t.ned.per1, freq.ned.per2 = sum.t.ned.per2,#cluster\n",
    "                                                                                             freq.ned = sum.t.ned, freq.bel = sum.t.bel,\n",
    "                                                                                             u.bel.per1, u.bel.per2, u.ned.per1, u.ned.per2,\n",
    "                                                                                             u.bel.inf.for.1, u.bel.inf.for.2, u.ned.inf.for.1, u.ned.inf.for.2,\n",
    "                                                                                             iu.bel.formal.per1, iu.bel.formal.per2, iu.ned.formal.per1, iu.ned.formal.per2))\n",
    "                                                                                             }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.concepts.clusters.demo <- merge(d.concepts.clusters.demo, \n",
    "                                  d.conceptsvariants[!duplicated(d.conceptsvariants$concept),c('concept','partofspeech','semfield')], \n",
    "                                  by=\"concept\")\n",
    "\n",
    "d.concepts.clusters.demo$Standardization.bel <- d.concepts.clusters.demo$u.bel.per2 - d.concepts.clusters.demo$u.bel.per1 \n",
    "d.concepts.clusters.demo$Standardization.ned <- d.concepts.clusters.demo$u.ned.per2 - d.concepts.clusters.demo$u.ned.per1 \n",
    "\n",
    "d.concepts.clusters.demo$Formalization.bel <- d.concepts.clusters.demo$u.bel.inf.for.2 - d.concepts.clusters.demo$u.bel.inf.for.1 \n",
    "d.concepts.clusters.demo$Formalization.ned <- d.concepts.clusters.demo$u.ned.inf.for.2  - d.concepts.clusters.demo$u.ned.inf.for.1 \n",
    "\n",
    "d.concepts.clusters.demo$Homogeneization.bel <- d.concepts.clusters.demo$iu.bel.formal.per2 - d.concepts.clusters.demo$iu.bel.formal.per1\n",
    "d.concepts.clusters.demo$Homogeneization.ned <- d.concepts.clusters.demo$iu.ned.formal.per2  - d.concepts.clusters.demo$iu.ned.formal.per1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_count_proportion_merged <- merge(d.concepts.clusters.demo, concept_count_proportions, by=\"concept\")\n",
    "\n",
    "concept_count_proportion_merged$freq.bel <- concept_count_proportion_merged$freq.bel.per1 + concept_count_proportion_merged$freq.bel.per2\n",
    "concept_count_proportion_merged$freq.ned <- concept_count_proportion_merged$freq.ned.per1 + concept_count_proportion_merged$freq.ned.per2\n",
    "\n",
    "concept_count_proportion_merged$proportion_in <- concept_count_proportion_merged$tot.tokens.cluster/concept_count_proportion_merged$sample_size\n",
    "concept_count_proportion_merged$proportion_in.bel <- concept_count_proportion_merged$freq.bel/concept_count_proportion_merged$sample_size\n",
    "concept_count_proportion_merged$proportion_in.ned <- concept_count_proportion_merged$freq.ned/concept_count_proportion_merged$sample_size\n",
    "concept_count_proportion_merged$proportion_in_bel.per1 <- concept_count_proportion_merged$freq.bel.per1/concept_count_proportion_merged$sample_size\n",
    "concept_count_proportion_merged$proportion_in_bel.per2 <- concept_count_proportion_merged$freq.bel.per2/concept_count_proportion_merged$sample_size\n",
    "concept_count_proportion_merged$proportion_in_ned.per1 <- concept_count_proportion_merged$freq.ned.per1/concept_count_proportion_merged$sample_size\n",
    "concept_count_proportion_merged$proportion_in_ned.per2 <- concept_count_proportion_merged$freq.ned.per2/concept_count_proportion_merged$sample_size\n",
    "\n",
    "\n",
    "concept_count_proportion_merged$final_size <- round(concept_count_proportion_merged$proportion_in * concept_count_proportion_merged$original_size)\n",
    "concept_count_proportion_merged$final_size_bel <- round(concept_count_proportion_merged$proportion_in.bel * concept_count_proportion_merged$original_size)\n",
    "concept_count_proportion_merged$final_size_ned <- round(concept_count_proportion_merged$proportion_in.ned * concept_count_proportion_merged$original_size)\n",
    "concept_count_proportion_merged$final_size_bel.per1 <- round(concept_count_proportion_merged$proportion_in_bel.per1 * concept_count_proportion_merged$original_size)\n",
    "concept_count_proportion_merged$final_size_bel.per2 <- round(concept_count_proportion_merged$proportion_in_bel.per2 * concept_count_proportion_merged$original_size)\n",
    "concept_count_proportion_merged$final_size_ned.per1 <- round(concept_count_proportion_merged$proportion_in_ned.per1 * concept_count_proportion_merged$original_size)\n",
    "concept_count_proportion_merged$final_size_ned.per2 <- round(concept_count_proportion_merged$proportion_in_ned.per2 * concept_count_proportion_merged$original_size)\n",
    "\n",
    "concept_count_proportion_merged$parameters <- gsub(\".+?(cw.+)\",\"\\\\1\", as.character(concept_count_proportion_merged$model.name))\n",
    "                                          \n",
    "dTemp <- concept_count_proportion_merged %>%\n",
    "group_by(parameters) %>%\n",
    "summarise(final_proportion = final_size/sum(final_size), \n",
    "          final_proportion.bel = final_size_bel/sum(final_size_bel),\n",
    "          final_proportion.ned = final_size_ned/sum(final_size_ned),\n",
    "          final_proportion.bel.per1 = final_size_bel.per1/sum(final_size_bel.per1),\n",
    "          final_proportion.bel.per2 = final_size_bel.per2/sum(final_size_bel.per2),\n",
    "          final_proportion.ned.per1 = final_size_ned.per1/sum(final_size_ned.per1),\n",
    "          final_proportion.ned.per2 = final_size_ned.per2/sum(final_size_ned.per2),\n",
    "          model.name)\n",
    "\n",
    "concept_count_proportion_merged2 <- merge(concept_count_proportion_merged, dTemp , by=\"model.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_count_proportion_merged2 %>% summarize(\n",
    "                                      unweightedMean=mean(Formalization.bel, na.rm=T),\n",
    "                                      unweightedSd = sd(Formalization.bel, na.rm=T),\n",
    "                                      unweightedSe = sd(Formalization.bel, na.rm=T)/sqrt(nrow(.)),\n",
    "    \n",
    "                                      weightedMean.bel=weighted.mean(Formalization.bel, final_proportion.bel, na.rm=T),\n",
    "                                      weightedSd.bel=sqrt(Hmisc::wtd.var(Formalization.bel, final_proportion.bel, method=\"ML\")),\n",
    "                                      weightedSe.bel=sqrt(Hmisc::wtd.var(Formalization.bel, final_proportion.bel, method=\"ML\"))/sqrt(nrow(.)),\n",
    "    \n",
    "                                      weightedMean.bel.per1=weighted.mean(Formalization.bel, final_proportion.bel.per1, na.rm=T),\n",
    "                                      weightedSd.bel.per1=sqrt(Hmisc::wtd.var(Formalization.bel, final_proportion.bel.per1, method=\"ML\")),\n",
    "                                      weightedSe.bel.per1=sqrt(Hmisc::wtd.var(Formalization.bel, final_proportion.bel.per1, method=\"ML\"))/sqrt(nrow(.)),\n",
    "\n",
    "                                      weightedMean.bel.per2=weighted.mean(Formalization.bel, final_proportion.bel.per2, na.rm=T),\n",
    "                                      weightedSd.bel.per2=sqrt(Hmisc::wtd.var(Formalization.bel, final_proportion.bel.per2, method=\"ML\")),\n",
    "                                      weightedSe.bel.per2=sqrt(Hmisc::wtd.var(Formalization.bel, final_proportion.bel.per2, method=\"ML\"))/sqrt(nrow(.)))\n",
    "\n",
    "concept_count_proportion_merged2 %>% summarize(\n",
    "                                      unweightedMean=mean(Formalization.ned, na.rm=T),\n",
    "                                      unweightedSd = sd(Formalization.ned, na.rm=T),\n",
    "                                      unweightedSe = sd(Formalization.ned, na.rm=T)/sqrt(nrow(.)),\n",
    "    \n",
    "                                      weightedMean.ned=weighted.mean(Formalization.ned, final_proportion.ned, na.rm=T),\n",
    "                                      weightedSd.ned=sqrt(Hmisc::wtd.var(Formalization.ned, final_proportion.ned, method=\"ML\")),\n",
    "                                      weightedSe.ned=sqrt(Hmisc::wtd.var(Formalization.ned, final_proportion.ned, method=\"ML\"))/sqrt(nrow(.)),\n",
    "    \n",
    "                                      weightedMean.ned.per1=weighted.mean(Formalization.ned, final_proportion.ned.per1, na.rm=T),\n",
    "                                      weightedSd.ned.per1=sqrt(Hmisc::wtd.var(Formalization.ned, final_proportion.ned.per1, method=\"ML\")),\n",
    "                                      weightedSe.ned.per1=sqrt(Hmisc::wtd.var(Formalization.ned, final_proportion.ned.per1, method=\"ML\"))/sqrt(nrow(.)),\n",
    "\n",
    "                                      weightedMean.ned.per2=weighted.mean(Formalization.ned, final_proportion.ned.per2, na.rm=T),\n",
    "                                      weightedSd.ned.per2=sqrt(Hmisc::wtd.var(Formalization.ned, final_proportion.ned.per2, method=\"ML\")),\n",
    "                                      weightedSe.ned.per2=sqrt(Hmisc::wtd.var(Formalization.ned, final_proportion.ned.per2, method=\"ML\"))/sqrt(nrow(.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_count_proportion_merged2$parameters.x <- as.factor(concept_count_proportion_merged2$parameters.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectometric measurements per concept (Figures 9.3, 9.4, 9.8, 9.9, 9.13, 9.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phen = c(\"Standardization.bel\") # Formalization.bel, Homogeneization.bel\n",
    "weight = c(\"final_size_bel\")\n",
    "\n",
    "dtemp <- concept_count_proportion_merged2 %>% group_by(concept) %>% summarize(unweightedMean=mean(get(phen), na.rm=T),\n",
    "                                      unweightedSd = sd(get(phen), na.rm=T),\n",
    "                                      unweightedSe = sd(get(phen), na.rm=T)/sqrt(nrow(.)),\n",
    "                                      weightedMean=weighted.mean(get(phen), get(weight), na.rm=T),\n",
    "                                      weightedSd=sqrt(Hmisc::wtd.var(get(phen), get(weight), method=\"unbiased\", na.rm=T)),\n",
    "                                      weightedSe=sqrt(Hmisc::wtd.var(get(phen), get(weight), method=\"unbiased\", na.rm=T))/sqrt(nrow(.)),\n",
    "#                                       simple.boot <- Hmisc::smean.cl.boot(Formalization.bel)\n",
    "#                                       simple.boot.mean=Hmisc::smean.cl.boot(Formalization.bel)[[1]],\n",
    "#                                       simple.boot.ci.lower=Hmisc::smean.cl.boot(Formalization.bel)[[2]],\n",
    "#                                       simple.boot.ci.upper=Hmisc::smean.cl.boot(Formalization.bel)[[3]]\n",
    "                                                                                  ) %>% mutate(concept = fct_reorder(concept, weightedMean))\n",
    "\n",
    "(meanUM <- mean(dtemp$unweightedMean, na.rm=T))\n",
    "(meanWM <- mean(dtemp$weightedMean, na.rm=T))\n",
    "\n",
    "ggplot(dtemp, aes(y=concept, x=weightedMean)) +  geom_point(size=3, color=\"red\") + geom_point(aes(y=concept, x=unweightedMean), size=3, color=\"blue\") + \n",
    "geom_errorbar(aes(xmin=weightedMean-weightedSd, xmax=weightedMean+weightedSd), size=1) +\n",
    "geom_vline(xintercept = 0, color=\"darkgreen\", size=2) + \n",
    "xlim(-1, 1.1) + \n",
    "geom_vline(xintercept=meanUM, color=\"blue\", size=2) + \n",
    "geom_vline(xintercept=meanWM, color=\"red\", size=2) + \n",
    "theme(axis.text = element_text(size=20), \n",
    "      axis.title = element_blank(),\n",
    "      strip.text.x = element_text(size = 40),\n",
    "      axis.text.x = element_text(size = 40),\n",
    "      legend.text = element_text(size=40),\n",
    "      legend.title = element_text(size=40),\n",
    "      plot.title = element_text(size=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phen = c(\"Homogeneization.ned\") # Standardization.ned, Formalization.ned\n",
    "weight = c(\"final_size_ned\")\n",
    "\n",
    "dtemp <- concept_count_proportion_merged2 %>% group_by(concept) %>% summarize(unweightedMean=mean(get(phen), na.rm=T),\n",
    "                                      unweightedSd = sd(get(phen), na.rm=T),\n",
    "                                      unweightedSe = sd(get(phen), na.rm=T)/sqrt(nrow(.)),\n",
    "                                      weightedMean=weighted.mean(get(phen), get(weight), na.rm=T),\n",
    "                                      weightedSd=sqrt(Hmisc::wtd.var(get(phen), get(weight), method=\"unbiased\", na.rm=T)),\n",
    "                                      weightedSe=sqrt(Hmisc::wtd.var(get(phen), get(weight), method=\"unbiased\", na.rm=T))/sqrt(nrow(.)),\n",
    "                                                                                  ) %>% mutate(concepts = fct_reorder(concept, weightedMean))\n",
    "\n",
    "(meanUM <- mean(dtemp$unweightedMean, na.rm=T))\n",
    "(meanWM <- mean(dtemp$weightedMean, na.rm=T))\n",
    "\n",
    "dtemp <- dtemp[!is.na(dtemp$unweightedMean),]\n",
    "\n",
    "ggplot(dtemp, aes(y=concepts, x=weightedMean)) +  geom_point(size=3, color=\"red\") + geom_point(aes(y=concepts, x=unweightedMean), size=3, color=\"blue\") + \n",
    "geom_errorbar(aes(xmin=weightedMean-weightedSd, xmax=weightedMean+weightedSd), size=1) +\n",
    "geom_vline(xintercept = 0, color=\"darkgreen\", size=2) + \n",
    "xlim(-0.8, 1.1) + \n",
    "geom_vline(xintercept=meanUM, color=\"blue\", size=2) + \n",
    "geom_vline(xintercept=meanWM, color=\"red\", size=2) + \n",
    "theme(axis.text = element_text(size=20), \n",
    "      axis.title = element_blank(),\n",
    "      strip.text.x = element_text(size = 40),\n",
    "      axis.text.x = element_text(size = 40),\n",
    "      legend.text = element_text(size=40),\n",
    "      legend.title = element_text(size=40),\n",
    "      plot.title = element_text(size=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectometric measurements per model (Figures 9.1, 9.2, 9.6, 9.7, 9.11, 9.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phen = c(\"Homogeneization.bel\") # Standardization.bel, Formalization.bel\n",
    "weight = c(\"final_size_bel\") # final_size_ned\n",
    "\n",
    "dtemp <- concept_count_proportion_merged2 %>% group_by(parameters.x) %>% summarize(unweightedMean=mean(get(phen), na.rm=T),\n",
    "                                      unweightedSd = sd(get(phen), na.rm=T),\n",
    "                                      unweightedSe = sd(get(phen), na.rm=T)/sqrt(nrow(.)),\n",
    "                                      weightedMean=weighted.mean(get(phen), get(weight), na.rm=T),\n",
    "                                      weightedSd=sqrt(Hmisc::wtd.var(get(phen), get(weight), method=\"unbiased\", na.rm=T)),\n",
    "                                      weightedSe=sqrt(Hmisc::wtd.var(get(phen), get(weight), method=\"unbiased\", na.rm=T))/sqrt(nrow(.)),\n",
    "                                                                                  ) %>% mutate(parameters.x = fct_reorder(parameters.x, weightedMean))\n",
    "\n",
    "(meanUM <- mean(dtemp$unweightedMean))\n",
    "(meanWM <- mean(dtemp$weightedMean))\n",
    "\n",
    "ggplot(dtemp, aes(y=parameters.x, x=weightedMean)) +  geom_point(size=3, color=\"red\") + geom_point(aes(y=parameters.x, x=unweightedMean), size=3, color=\"blue\") + \n",
    "geom_errorbar(aes(xmin=weightedMean-weightedSd, xmax=weightedMean+weightedSd), size=1) +\n",
    "geom_vline(xintercept = 0, color=\"darkgreen\", size=2) + \n",
    "xlim(-0.5, 0.5) + \n",
    "geom_vline(xintercept=meanUM, color=\"blue\", size=2) + \n",
    "geom_vline(xintercept=meanWM, color=\"red\", size=2) + \n",
    "theme(#axis.text = element_text(size=10), \n",
    "      axis.ticks.y = element_line(size=3),\n",
    "      axis.text.y = element_blank(), \n",
    "      axis.title = element_blank(),\n",
    "      strip.text.x = element_text(size = 40),\n",
    "      axis.text.x = element_text(size = 40),\n",
    "      legend.text = element_text(size=40),\n",
    "      legend.title = element_text(size=40),\n",
    "      plot.title = element_text(size=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phen = c(\"Homogeneization.ned\") # Standardization.ned, Formalization.ned\n",
    "weight = c(\"final_size_ned\")\n",
    "\n",
    "dtemp <- concept_count_proportion_merged2 %>% group_by(parameters.x) %>% summarize(unweightedMean=mean(get(phen), na.rm=T),\n",
    "                                      unweightedSd = sd(get(phen), na.rm=T),\n",
    "                                      unweightedSe = sd(get(phen), na.rm=T)/sqrt(nrow(.)),\n",
    "                                      weightedMean=weighted.mean(get(phen), get(weight), na.rm=T),\n",
    "                                      weightedSd=sqrt(Hmisc::wtd.var(get(phen), get(weight), method=\"unbiased\", na.rm=T)),\n",
    "                                      weightedSe=sqrt(Hmisc::wtd.var(get(phen), get(weight), method=\"unbiased\", na.rm=T))/sqrt(nrow(.)),\n",
    "                                                                                  ) %>% mutate(parameters.x = fct_reorder(parameters.x, weightedMean))\n",
    "\n",
    "(meanUM <- mean(dtemp$unweightedMean))\n",
    "(meanWM <- mean(dtemp$weightedMean))\n",
    "\n",
    "ggplot(dtemp, aes(y=parameters.x, x=weightedMean)) +  geom_point(size=3, color=\"red\") + geom_point(aes(y=parameters.x, x=unweightedMean), size=3, color=\"blue\") + \n",
    "geom_errorbar(aes(xmin=weightedMean-weightedSd, xmax=weightedMean+weightedSd), size=1) +\n",
    "geom_vline(xintercept = 0, color=\"darkgreen\", size=2) + \n",
    "xlim(-0.5, 0.5) + \n",
    "geom_vline(xintercept=meanUM, color=\"blue\", size=2) + \n",
    "geom_vline(xintercept=meanWM, color=\"red\", size=2) + \n",
    "theme(#axis.text = element_text(size=10), \n",
    "      axis.ticks.y = element_line(size=3),\n",
    "      axis.text.y = element_blank(), \n",
    "      axis.title = element_blank(),\n",
    "      strip.text.x = element_text(size = 40),\n",
    "      axis.text.x = element_text(size = 40),\n",
    "      legend.text = element_text(size=40),\n",
    "      legend.title = element_text(size=40),\n",
    "      plot.title = element_text(size=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression analysis for destandardization phenomena (Figures 9.5, 9.10, 9.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measvars = c(\"Formalization.bel\",\"Formalization.ned\")\n",
    "# measvars = c(\"Standardization.bel\",\"Standardization.ned\")\n",
    "measvars = c(\"Homogeneization.bel\",\"Homogeneization.ned\")\n",
    "\n",
    "dreg <- reshape2::melt(concept_count_proportion_merged2[,c(\"concept\",\"semfield\",\"final_size\", \"partofspeech\",\"cw\",\"foc_contribution\",\"ass_foc\",\"soc_selection\",\"svd\",\"foc_selection\",\"parameters.x\", measvars)], measure.vars=measvars)\n",
    "\n",
    "dreg <- dreg %>% mutate(variable = fct_recode(variable, \"NED\" = measvars[2], \"BEL\" = measvars[1]))\n",
    "# summary(na.omit(dreg))\n",
    "mod1 <- lmer(value~(semfield+cw+foc_contribution+ass_foc+soc_selection+svd+foc_selection+log(final_size))*variable+(1|parameters.x)+(1|concept), data=na.omit(dreg))\n",
    "\n",
    "# summary(dreg)\n",
    "# mod1 <- lmer(value~(partofspeech+cw+foc_contribution+ass_foc+soc_selection+svd+foc_selection+log(final_size))*variable+(1|parameters.x)+(1|concept), data=dreg)\n",
    "\n",
    "# step(mod1)\n",
    "\n",
    "\n",
    "## -- model for Formalization (nouns)\n",
    "# mod.final <- lmer(value ~ semfield + log(final_size) + variable + (1 | concept) + semfield:variable, data=na.omit(dreg))\n",
    "\n",
    "## -- model for Standardization (nouns)\n",
    "# mod.final <- lmer(value ~ semfield + cw + ass_foc + variable + (1 | concept) + \n",
    "#    semfield:variable + cw:variable, data=na.omit(dreg))\n",
    "\n",
    "## -- model for Heterogeneization (nouns)\n",
    "mod.final <- lmer(value ~ semfield + soc_selection + log(final_size) + variable + \n",
    "   (1 | concept) + semfield:variable + log(final_size):variable, data=na.omit(dreg))\n",
    "\n",
    "## -- model for Formalization (all)\n",
    "# mod.final <- lmer(value ~ partofspeech + foc_contribution + log(final_size) + variable + \n",
    "#     (1 | concept) + partofspeech:variable + foc_contribution:variable, data=dreg)\n",
    "\n",
    "## -- model for Standardization (all)\n",
    "# mod.final <- lmer(value ~ partofspeech + cw + soc_selection + log(final_size) + \n",
    "#     variable + (1 | concept) + partofspeech:variable + cw:variable + \n",
    "#     log(final_size):variable, data=dreg)\n",
    "\n",
    "## -- model for Heterogeneization (all)\n",
    "# mod.final <- lmer(value ~ cw + foc_contribution + soc_selection + log(final_size) + \n",
    "#    variable + (1 | concept) + cw:variable + log(final_size):variable, data=dreg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsm <- emmeans(mod.final, pairwise~semfield|variable, type=\"response\", weights=\"proportional\")$emmeans\n",
    "\n",
    "lsm.sum <- summary(lsm)\n",
    "\n",
    "ggplot(lsm.sum, aes(semfield, emmean,colour=variable,fill=variable)) + \n",
    "  geom_hline(aes(yintercept=0),linetype=\"dashed\",colour=\"red\") + \n",
    "  geom_bar(stat=\"identity\", width = 0.7, position = position_dodge()) + \n",
    "  geom_errorbar(aes(ymin=asymp.LCL, ymax=asymp.UCL), width=0.7, position=position_dodge(),colour=\"black\") +\n",
    " labs(x=\"semantic field\") + coord_cartesian(ylim = c(-0.3,0.4)) +\n",
    "  theme(axis.title.y=element_blank(), legend.position=c(0.10,0.90)) +\n",
    "  scale_fill_manual(values=c(\"red\",\"orange\")) + scale_color_manual(values=c(\"red\",\"orange\")) + \n",
    "theme(axis.text = element_text(size=40), \n",
    "      axis.title = element_text(size=40),\n",
    "      strip.text.x = element_text(size = 40),\n",
    "      legend.text = element_text(size=40),\n",
    "      legend.title = element_text(size=40))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
